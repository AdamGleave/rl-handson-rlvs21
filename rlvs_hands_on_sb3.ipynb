{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rlvs_hands_on_sb3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 Hands-on Session - RLVS\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
        "You will also learn how to define a gym wrapper and callback to customise the training.\n",
        "We will finish this session by trying out multiprocessing.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khNkrgcI6Z1"
      },
      "source": [
        "# Part I: Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor: \n",
        "\n",
        "```PPO(\"MlpPolicy\", env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "source": [
        "from stable_baselines3.ppo.policies import MlpPolicy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "\n",
        "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo2.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "source": [
        "# Create the gym Env\n",
        "env = gym.make('CartPole-v1')\n",
        "# Create the RL agent\n",
        "model = PPO(MlpPolicy, env, verbose=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0K2YzpGDwQ4"
      },
      "source": [
        "### Using the model to predict actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRfb_f7uD-H9",
        "outputId": "4698d6f4-eed4-4aa7-b5d7-ed76c3618620"
      },
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Discrete(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF_xsDEADvv_"
      },
      "source": [
        "# Retrieve first observation\n",
        "obs = env.reset()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgrRoM23EU9Z"
      },
      "source": [
        "action, _states = model.predict(obs, deterministic=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1wRSvPrEjUa",
        "outputId": "a70bb00d-eb0b-49d5-8533-90a4cc89aa53"
      },
      "source": [
        "print(action)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtEtKqKEvFC"
      },
      "source": [
        "Step in the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmJ-yv5MEllY"
      },
      "source": [
        "obs, reward, done, infos = env.step(action)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTNOOlOdE1lH",
        "outputId": "612f6db9-6cac-4e75-8cbc-eedf7ddcea5c"
      },
      "source": [
        "print(f\"obs_shape={obs.shape}, reward={reward}, done? {done}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "obs_shape=(4,), reward=1.0, done? False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iSeXppUFNLg"
      },
      "source": [
        "### Exercise (10 minutes): write the function to evaluate the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf7Ktha2FE3B"
      },
      "source": [
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: BaseAlgorithm,\n",
        "    env: gym.Env,\n",
        "    num_episodes: int = 100,\n",
        "    deterministic: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate an RL agent for `num_episodes`.\n",
        "\n",
        "    :param model: the RL Agent\n",
        "    :param env: the gym Environment\n",
        "    :param num_episodes: number of episodes to evaluate it\n",
        "    :param deterministic: Whether to use deterministic or stochastic actions\n",
        "    :return: Mean reward for the last `num_episodes`\n",
        "    \"\"\"\n",
        "    n_episodes = 0\n",
        "    episode_reward = 0.0\n",
        "    episode_rewards = []\n",
        "    obs = env.reset()\n",
        "\n",
        "    while n_episodes < num_episodes:\n",
        "        \n",
        "        action, _ = model.predict(obs, deterministic=deterministic)\n",
        "        obs, reward, done, infos = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Not needed when using `VecEnv`\n",
        "        if done:\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0.0\n",
        "            n_episodes += 1\n",
        "            obs = env.reset()\n",
        "\n",
        "    mean_episode_reward = np.mean(episode_rewards)\n",
        "    print(f\"Mean reward: {mean_episode_reward} Num episodes: {num_episodes}\")\n",
        "\n",
        "    return mean_episode_reward"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3SahgtZH2TX"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XGmJK4oHUeT",
        "outputId": "a6adfab6-46cc-4ced-a4cb-e1fa89eb2ee8"
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, env, num_episodes=100, deterministic=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward: 23.0 Num episodes: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjjPxrwkYJ2i"
      },
      "source": [
        "Stable-Baselines already provides you with that helper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z6K9YImYJEx"
      },
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lHSorUyH8I8"
      },
      "source": [
        "# The Monitor wrapper allows to keep track of the training reward and other infos (useful for plotting)\n",
        "env = Monitor(env)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oPTHjxyZSOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e86e43-b59a-4c6a-8bd2-7d5fe883a906"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:150.62 +/- 29.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix='', video_folder='videos/'):\n",
        "  \"\"\"\n",
        "  :param env_id: (str)\n",
        "  :param model: (RL model)\n",
        "  :param video_length: (int)\n",
        "  :param prefix: (str)\n",
        "  :param video_folder: (str)\n",
        "  \"\"\"\n",
        "  eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
        "  # Start the video at step=0 and record 500 steps\n",
        "  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n",
        "                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n",
        "                              name_prefix=prefix)\n",
        "\n",
        "  obs = eval_env.reset()\n",
        "  for _ in range(video_length):\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "  # Close the video recorder\n",
        "  eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "source": [
        "record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "source": [
        "show_videos('videos', prefix='ppo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5WHto5hNaZN"
      },
      "source": [
        "### Exercise (5 minutes): Save, Load The Model and that the loading was correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCcS6wJ6Nurd"
      },
      "source": [
        "# Sample obvservations\n",
        "observations = np.array([env.observation_space.sample() for _ in range(10)])\n",
        "# Predict action using trained model\n",
        "action_before_saving, _ = model.predict(observations, deterministic=True)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWCUZSD7NZ_Y"
      },
      "source": [
        "# Save the model\n",
        "model.save(\"ppo_cartpole\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ne1MlIiNpee",
        "outputId": "7e95c3ea-1b0e-4516-d7a6-2b594e1536cf"
      },
      "source": [
        "!ls *.zip"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ppo_cartpole.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55G-6nJLNrPk"
      },
      "source": [
        "# Load the model\n",
        "model = PPO.load(\"ppo_cartpole\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIWII5YxNtrI"
      },
      "source": [
        "# Predict with the loaded model\n",
        "action_after_loading, _ = model.predict(observations, deterministic=True)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m06ipphZN_0E"
      },
      "source": [
        "# Check that the predictions are the same\n",
        "assert np.allclose(action_before_saving, action_after_loading), \"Somethng went wrong in the loading\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line\n",
        "\n",
        "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm4RncfCJFyP"
      },
      "source": [
        "# Part II: Gym Wrappers\n",
        "\n",
        "\n",
        "In this part, you will learn how to use *Gym Wrappers* which allow to do monitoring, normalization, limit the number of steps, feature augmentation, ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4AAfmISQIA"
      },
      "source": [
        "## Anatomy of a gym wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnTS9e9hTzZZ"
      },
      "source": [
        "A gym wrapper follows the [gym](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html) interface: it has a `reset()` and `step()` method.\n",
        "\n",
        "Because a wrapper is *around* an environment, we can access it with `self.env`, this allow to easily interact with it without modifying the original env.\n",
        "There are many wrappers that have been predefined, for a complete list refer to [gym documentation](https://github.com/openai/gym/tree/master/gym/wrappers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYo0C0TQSL3c"
      },
      "source": [
        "class CustomWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env:  Gym environment that will be wrapped\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "  \n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Reset the environment \n",
        "    \"\"\"\n",
        "    obs = self.env.reset()\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"\n",
        "    :param action: ([float] or int) Action taken by the agent\n",
        "    :return: (np.ndarray, float, bool, dict) observation, reward, is the episode over?, additional informations\n",
        "    \"\"\"\n",
        "    obs, reward, done, info = self.env.step(action)\n",
        "    return obs, reward, done, info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7RkTLshPMGq"
      },
      "source": [
        "### Exercise (7 minutes): limit the episode length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h_ypIxgPLGL"
      },
      "source": [
        "class TimeLimitWrapper(gym.Wrapper):\n",
        "  \"\"\"\n",
        "  :param env: Gym environment that will be wrapped\n",
        "  :param max_steps: Max number of steps per episode\n",
        "  \"\"\"\n",
        "  def __init__(self, env: gym.Env, max_steps: int = 100):\n",
        "    # Call the parent constructor, so we can access self.env later\n",
        "    super().__init__(env)\n",
        "    self.max_steps = max_steps\n",
        "    # Counter of steps per episode\n",
        "    self.episode_steps = 0\n",
        "  \n",
        "  def reset(self):\n",
        "    # Reset the counter\n",
        "    self.episode_steps = 0\n",
        "    return self.env.reset()\n",
        "\n",
        "  def step(self, action):\n",
        "    obs, reward, done, infos = self.env.step(action)\n",
        "    # Increment the counter\n",
        "    self.episode_steps += 1\n",
        "    # Overwrite the done signal when time limit is reached \n",
        "    if self.episode_steps >= self.max_steps:\n",
        "      done = True\n",
        "      # Update the info dict to signal that the time limit was exceeded\n",
        "      infos[\"episode_timeout\"] = True\n",
        "    return obs, reward, done, infos"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FQptLSuPB3A"
      },
      "source": [
        "#### Test the wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szZ43D5PVB07"
      },
      "source": [
        "from gym.envs.classic_control.pendulum import PendulumEnv\n",
        "\n",
        "# Here we create the environment directly because gym.make() already wrap the environement in a TimeLimit wrapper otherwise\n",
        "env = PendulumEnv()\n",
        "# Wrap the environment\n",
        "env = TimeLimitWrapper(env, max_steps=100)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cencka9iVg9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4287da53-5f1f-4bdd-ea68-c4017ba6b7ce"
      },
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "n_steps = 0\n",
        "while not done:\n",
        "  # Take random actions\n",
        "  random_action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(random_action)\n",
        "  n_steps += 1\n",
        "\n",
        "print(n_steps, info)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 {'episode_timeout': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkMYA63sV9aA"
      },
      "source": [
        "In practice, `gym` already have a wrapper for that named `TimeLimit` (`gym.wrappers.TimeLimit`) that is used by most environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yms4z_GaQ6dz"
      },
      "source": [
        "# Part III: Callbacks\n",
        "\n",
        "In this part, you will learn how to use [Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html) which allow to do monitoring, auto saving, model manipulation, progress bars, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHk8FXdRUnw"
      },
      "source": [
        "Please read the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html). Although Stable-Baselines3 provides you with a callback collection (e.g. for creating checkpoints or for evaluation), we are going to re-implement some so you can get a good understanding of how they work.\n",
        "\n",
        "To build a custom callback, you need to create a class that derives from `BaseCallback`. This will give you access to events (`_on_training_start`, `_on_step()`) and useful variables (like `self.model` for the RL model).\n",
        "\n",
        "`_on_step` returns a boolean value for whether or not the training should continue.\n",
        "\n",
        "Thanks to the access to the models variables, in particular `self.model`, we are able to even change the parameters of the model without halting the training, or changing the model's code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE30k2i7kohh"
      },
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjRvJ8zBftL3"
      },
      "source": [
        "class CustomCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that derives from ``BaseCallback``.\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(CustomCallback, self).__init__(verbose)\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseRLModel\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = None  # type: Dict[str, Any]\n",
        "        # self.globals = None  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger = None  # type: logger.Logger\n",
        "        # # Sometimes, for event callback, it is useful\n",
        "        # # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqpPtxaCfynB"
      },
      "source": [
        "Here we have a simple callback that can only be called twice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILY0AkFfzPJ"
      },
      "source": [
        "class SimpleCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    a simple callback that can only be called twice\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(SimpleCallback, self).__init__(verbose)\n",
        "        self._called = False\n",
        "    \n",
        "    def _on_step(self):\n",
        "      if not self._called:\n",
        "        print(\"callback - first call\")\n",
        "        self._called = True\n",
        "        return True # returns True, training continues.\n",
        "      print(\"callback - second call\")\n",
        "      return False # returns False, training stops.      "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gTXaNLARUnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4332aec-c277-4664-9550-5452e9e716fb"
      },
      "source": [
        "model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)\n",
        "model.learn(8000, callback=SimpleCallback())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'Pendulum-v0'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "callback - first call\n",
            "callback - second call\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7f70db60d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adsKMvDkRUn0"
      },
      "source": [
        "## Exercise (8 minutes): Checkpoint Callback\n",
        "\n",
        "In RL, it is quite useful to save checkpoints during, as we can end up with burn-in of a bad policy. It also useful if you want to see the progression over time.\n",
        "\n",
        "This is a typical use case for callback, as they can call the save function of the model, and observe the training over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDI3lKTiiKP9"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzMHj7r3h78m"
      },
      "source": [
        "class CheckpointCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Callback for saving a model every ``save_freq`` steps\n",
        "\n",
        "    :param save_freq:\n",
        "    :param save_path: Path to the folder where the model will be saved.\n",
        "    :param name_prefix: Common prefix to the saved models\n",
        "    :param verbose: Whether to print additional infos or not\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, save_freq: int, save_path: str, name_prefix: str = \"rl_model\", verbose: int = 0):\n",
        "        super().__init__(verbose)\n",
        "        self.save_freq = save_freq\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "\n",
        "    def _init_callback(self) -> None:\n",
        "        # Create folder if needed\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.save_freq == 0:\n",
        "          # Name of the checkpoint\n",
        "          checkpoint_path = os.path.join(self.save_path, f\"{self.name_prefix}_{self.num_timesteps}_steps\")\n",
        "          \n",
        "          if self.verbose > 0:\n",
        "            print(f\"Saving checkpoint to {checkpoint_path}.zip\")\n",
        "\n",
        "          self.model.save(checkpoint_path)\n",
        "        return True"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TuYLBEaRUn0"
      },
      "source": [
        "log_dir = \"/tmp/gym/\"\n",
        "# Create Callback\n",
        "callback = CheckpointCallback(save_freq=1000, save_path=\"/tmp/gym/\", verbose=1)\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=0)\n",
        "model.learn(total_timesteps=10000, callback=callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc_z8jF-TFQP"
      },
      "source": [
        "!ls \"/tmp/gym/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0_iiocKUTW0"
      },
      "source": [
        "Note: The `CheckpointCallback` as well as other [common callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html), like the `EvalCallback` are already included in Stable-Baselines3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4TavoikVlCt"
      },
      "source": [
        "## Multiprocessing Demo\n",
        "\n",
        "\n",
        "[Vectorized Environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html) are a method for stacking multiple independent environments into a single environment. Instead of training an RL agent on 1 environment per step, it allows us to train it on n environments per step. This provides two benefits:\n",
        "* Agent experience can be collected more quickly\n",
        "* The experience will contain a more diverse range of states, it usually improves exploration\n",
        "\n",
        "Stable-Baselines provides two types of Vectorized Environment:\n",
        "- SubprocVecEnv which run each environment in a separate process\n",
        "- DummyVecEnv which run all environment on the same process\n",
        "\n",
        "In practice, DummyVecEnv is usually faster than SubprocVecEnv because of communication delays that subprocesses have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7obQMBgXV8_z"
      },
      "source": [
        "import time\n",
        "\n",
        "from stable_baselines3.common.env_util import make_vec_env"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1K_YuEfVnT6"
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "n_steps = 1024"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBa5rJfiWFX3",
        "outputId": "962518be-7a72-4465-c6ce-2e46ec7ec4e0"
      },
      "source": [
        "start_time_one_env = time.time()\n",
        "model = PPO(\"MlpPolicy\", env, n_epochs=1, n_steps=n_steps, verbose=1).learn(int(2e4))\n",
        "time_one_env = time.time() - start_time_one_env"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylbC5slbWzKo",
        "outputId": "d399ef37-0223-4008-ef62-4a5ff57aa021"
      },
      "source": [
        "print(f\"Took {time_one_env:.2f}s\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 22.75s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0UgmDOHWBWU",
        "outputId": "a880fa85-7aab-47f1-abe5-528b1285d52f"
      },
      "source": [
        "start_time_vec_env = time.time()\n",
        "# Create 16 environments\n",
        "vec_env = make_vec_env(\"Pendulum-v0\", n_envs=16)\n",
        "# At each call to `env.step()`, 16 transitions will be collected, so we account for that for fair comparison\n",
        "model = PPO(\"MlpPolicy\", vec_env, n_epochs=1, n_steps=n_steps // 16, verbose=1).learn(int(2e4))\n",
        "\n",
        "time_vec_env = time.time() - start_time_vec_env"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN3o5TJ3WxOy",
        "outputId": "146bb166-51eb-405c-da7c-aa190ee908d0"
      },
      "source": [
        "print(f\"Took {time_vec_env:.2f}s\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Took 4.04s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwFOp0j-ga-_"
      },
      "source": [
        "# Part IV: The importance of hyperparameter tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytOtL9GdmrE"
      },
      "source": [
        "When compared with Supervised Learning, Deep Reinforcement Learning is far more sensitive to the choice of hyper-parameters such as learning rate, number of neurons, number of layers, optimizer ... etc. \n",
        "Poor choice of hyper-parameters can lead to poor/unstable convergence. This challenge is compounded by the variability in performance across random seeds (used to initialize the network weights and the environment).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8PNN9kcgolk"
      },
      "source": [
        "### Challenge (15 minutes): \"Grad Student Descent\" - can you beat automatic tuning?\n",
        "\n",
        "The challenge is to find the best hyperparameters (max performance) for A2C on CartPole with a limited budget of 20 000 steps.\n",
        "\n",
        "You will compete against automatic hyperparameter tuning, good luck ;)\n",
        "\n",
        "\n",
        "Maximum reward: 500 on `CartPole-v1`\n",
        "\n",
        "The hyperparameters should work for different random seeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aqxsini7H3"
      },
      "source": [
        "budget = int(2e4)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDQ805DBi3KM"
      },
      "source": [
        "#### The baseline: default hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1PSNGcsi2dP",
        "outputId": "dec204ee-b5db-44f2-ea7f-90bce9b1038d"
      },
      "source": [
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1).learn(budget)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3X0G0ng2OE",
        "outputId": "505aea0e-4a09-4330-a230-df62bc1ea81b"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:173.96 +/- 23.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-fi1-oKnUI2"
      },
      "source": [
        "**Your goal is to beat that baseline and get closer to the optimal 500 episodic reward**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvq8zizok1X_"
      },
      "source": [
        "Time to tune!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaqCCH4gkRH_"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDUfeZcyjPKS",
        "outputId": "37306664-c438-4331-d1dd-0f8a8d2b39a0"
      },
      "source": [
        "policy_kwargs = dict(\n",
        "    net_arch=[\n",
        "      dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic\n",
        "    ],\n",
        "    ortho_init=True, # Orthogonal initialization,\n",
        "    activation_fn=nn.Tanh,\n",
        ")\n",
        "\n",
        "hyperparams = dict(\n",
        "    n_steps=5,\n",
        "    learning_rate=7e-4,\n",
        "    gamma=0.99, # discount factor\n",
        "    gae_lambda=1.0, # Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
        "                    # Equivalent to classic advantage when set to 1.\n",
        "    max_grad_norm=0.5, # The maximum value for the gradient clipping\n",
        "    ent_coef=0.0, # Entropy coefficient for the loss calculation\n",
        ")\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1, **hyperparams).learn(budget)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YHvzTJArTGU",
        "outputId": "28cec9a0-a00f-43e0-dbb9-267dd3dd45c1"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:171.36 +/- 24.66\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxlrPkl9ndaY"
      },
      "source": [
        "### Result from automatic hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQf6Xfa7lNQN",
        "outputId": "20abd6cd-c68d-4812-f956-d559043fbbb0"
      },
      "source": [
        "policy_kwargs = dict(\n",
        "    net_arch=[\n",
        "      dict(vf=[64], pi=[64]),\n",
        "    ],\n",
        "    ortho_init=False,\n",
        "    activation_fn=nn.Tanh,\n",
        ")\n",
        "\n",
        "hyperparams = dict(\n",
        "    n_steps=int(2 ** 6),\n",
        "    learning_rate=0.002,\n",
        "    gamma=0.999,\n",
        "    gae_lambda=0.88,\n",
        "    max_grad_norm=1.1,\n",
        ")\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=20, verbose=1, **hyperparams).learn(budget)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHXFBLvzgnxn",
        "outputId": "918bff44-67e0-4fc6-fb65-ed895d38c07e"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean_reward:422.24 +/- 83.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCbep6z1h3D1"
      },
      "source": [
        "Simple example of hyperparameter tuning: https://github.com/optuna/optuna/blob/master/examples/rl/sb3_simple.py\n",
        "\n",
        "Complete example: https://github.com/DLR-RM/rl-baselines3-zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUeYnfJVpB2"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "- SB3 101\n",
        "- Gym wrappers\n",
        "- SB3 callbacks\n",
        "- the importance of good hyperparameters\n",
        "- multiprocessing\n",
        "- more complete tutorial: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-gqIPXqV7zZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}